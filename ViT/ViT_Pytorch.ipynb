{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IMNkkxPZrphu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self,batch_size,num_patches,emb_size):\n",
        "\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.num_patches = num_patches\n",
        "    self.emb_size = emb_size\n",
        "\n",
        "\n",
        "    positions = np.arange(num_patches)[:,np.newaxis]\n",
        "    depth = np.arange(emb_size)[np.newaxis, :]\n",
        "    depth = (2*depth//2)/emb_size\n",
        "\n",
        "    angle_rates = 1 / (10000**depth)\n",
        "\n",
        "    angle_rads  = positions * angle_rates\n",
        "    angle_rads[:,0::2] = np.sin(angle_rads[:,0::2])\n",
        "    angle_rads[:,1::2] = np.cos(angle_rads[:,1::2])\n",
        "\n",
        "\n",
        "    self.positions = positions * angle_rads\n",
        "\n",
        "  def forward(self):\n",
        "    return torch.tensor(np.broadcast_to(self.positions,[self.batch_size,self.num_patches,self.emb_size]),dtype=torch.float32)\n",
        "     "
      ],
      "metadata": {
        "id": "1kwa1x4rt1AD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               img_size = 224,\n",
        "               batch_size=32,\n",
        "               patch_size=16,\n",
        "               emb_size=768,\n",
        "               ):\n",
        "    \n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.img_size = img_size\n",
        "    self.batch_size = batch_size\n",
        "    self.patch_size = patch_size\n",
        "    self.emb_size = emb_size\n",
        "\n",
        "    self.num_patches = (img_size * img_size) // patch_size**2\n",
        "\n",
        "    self.cnn_layer = nn.Conv2d(in_channels=3,out_channels=emb_size,kernel_size=self.patch_size,stride=self.patch_size)\n",
        "    self.flatten = nn.Flatten(start_dim=2,end_dim=3)\n",
        "\n",
        "    self.pos = PositionalEmbedding(batch_size,self.num_patches+1,emb_size)\n",
        "\n",
        "\n",
        "  def forward(self,images):\n",
        "    \n",
        "    # patch embedding\n",
        "    patches = self.cnn_layer(images)\n",
        "    patches = self.flatten(patches)\n",
        "    patches = patches.permute(0, 2, 1)\n",
        "\n",
        "    # class learnable embedding\n",
        "    class_token = nn.Parameter(torch.ones((self.batch_size,1,self.emb_size)),requires_grad=True)\n",
        "    \n",
        "    # concat class token with patch embedding\n",
        "    embedding = torch.cat((class_token, patches), dim=1)\n",
        "  \n",
        "    # Positional Embedding\n",
        "    pos_emb = nn.Parameter(self.pos(),requires_grad=True)\n",
        "  \n",
        "    \n",
        "    # Add positional emb with embeddings\n",
        "\n",
        "    embedding = embedding + pos_emb\n",
        "\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "L0IQcKcat09a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MultiHeaded Attention"
      ],
      "metadata": {
        "id": "LgnvMHQC_7Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 emb_size=768,\n",
        "                 batch_size=32,\n",
        "                 heads=12):\n",
        "        super(MultiHeadAttention,self).__init__()\n",
        "        \n",
        "        \n",
        "\n",
        "        self.emb_size= emb_size\n",
        "        self.heads=heads\n",
        "        self.head_dim = emb_size//heads\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # Queries, Keys and Values Matrices Layers\n",
        "        self.queries = nn.Linear(self.emb_size,self.emb_size)\n",
        "        self.keys = nn.Linear(self.emb_size,self.emb_size)\n",
        "        self.values = nn.Linear(self.emb_size,self.emb_size)\n",
        "        self.out_projection = nn.Linear(self.emb_size,self.emb_size) \n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    \n",
        "    \n",
        "    def self_attention(self,queries,keys,values,masked=False):\n",
        "        \n",
        "        scores = torch.matmul(queries,keys.transpose(-2,-1))\n",
        "        scores = scores/np.sqrt(self.head_dim)\n",
        " \n",
        "        \n",
        "        scores = self.softmax(scores)\n",
        "        atten = torch.matmul(scores,values)\n",
        "        \n",
        "        return atten \n",
        "    \n",
        "     \n",
        "    def forward(self,x):\n",
        "        \n",
        "        \n",
        "        # For Multiheaded Attention\n",
        "        queries = self.queries(x)\n",
        "        keys = self.keys(x)\n",
        "        values = self.values(x)\n",
        "        \n",
        "            \n",
        "        # Self Attention\n",
        "        attention = self.self_attention(queries,keys,values)\n",
        "            \n",
        "        # Last Projection Matrix \n",
        "        \n",
        "        out = self.out_projection(attention)\n",
        "                          \n",
        "        return out\n"
      ],
      "metadata": {
        "id": "lGITrqILAAAo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Block"
      ],
      "metadata": {
        "id": "CuvzzbwBA9bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBlock(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               batch_size=32,\n",
        "               num_patches=196,\n",
        "               emb_size=768,\n",
        "               mlp_block=3072):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm([batch_size,num_patches+1,emb_size])\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(emb_size,mlp_block),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(mlp_block,emb_size)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x = self.layer_norm(x)\n",
        "    x = self.mlp(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "DHrB-JNF__9_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfomer Block"
      ],
      "metadata": {
        "id": "7tAZBzvVDFOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               num_patches=196,\n",
        "               emb_size=768,\n",
        "               mlp_block=3072,\n",
        "               heads=12,\n",
        "               batch_size=32):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    self.ln1 = nn.LayerNorm([batch_size,num_patches+1,emb_size])\n",
        "    self.ln2 = nn.LayerNorm([batch_size,num_patches+1,emb_size])\n",
        "\n",
        "    self.mha = MultiHeadAttention(emb_size,batch_size,heads)\n",
        "    self.mlp = MLPBlock(batch_size,num_patches,emb_size,mlp_block)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    norm = self.ln1(x)\n",
        "\n",
        "    msa = self.mha(norm)\n",
        "\n",
        "    # skip connection\n",
        "\n",
        "    x = x + msa \n",
        "\n",
        "    norm = self.ln2(x)\n",
        "\n",
        "    mlp_layer = self.mlp(norm)\n",
        "\n",
        "    # skip connection\n",
        "\n",
        "    x = mlp_layer + x\n",
        "\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "bWXgJUgP__7o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transfomer"
      ],
      "metadata": {
        "id": "mnY3jDCNE-4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "\n",
        "  \n",
        "  def __init__(self,\n",
        "               img_size = 224,\n",
        "               batch_size=32,\n",
        "               patch_size=16,\n",
        "               emb_size=768,\n",
        "               heads=12,\n",
        "               mlp_block=3072,\n",
        "               encoder_layers = 12,\n",
        "               num_classes=10):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.emb_size = emb_size\n",
        "\n",
        "    self.num_patches = (img_size * img_size) // patch_size**2\n",
        "\n",
        "    # Patch Embedding\n",
        "    self.patch_emb = PatchEmbedding(img_size,batch_size,patch_size,emb_size)\n",
        "\n",
        "    # Transformer Block\n",
        "    self.encoder = [Transformer(self.num_patches,emb_size,mlp_block,heads,batch_size) for _ in range(encoder_layers)]\n",
        "\n",
        "    # global_avg_pool\n",
        "\n",
        "    self.global_avg_pool = nn.AdaptiveAvgPool2d((1,emb_size))\n",
        "\n",
        "    # Classifier\n",
        "    self.classifier = nn.Linear(emb_size,num_classes)\n",
        "  \n",
        "  \n",
        "  def forward(self,images):\n",
        "\n",
        "    x = self.patch_emb(images)\n",
        "\n",
        "    for enc_layer in self.encoder:\n",
        "\n",
        "      x = enc_layer(x)\n",
        "    \n",
        "    x = self.global_avg_pool(x)\n",
        "    x = torch.reshape(x,(self.batch_size,self.emb_size))\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x \n",
        "\n"
      ],
      "metadata": {
        "id": "HQKR7VUFE-n_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "\n",
        "vit = ViT()\n",
        "\n",
        "x = torch.rand((32,3,224,224))\n",
        "out = vit(x)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j832fcBrJd9S",
        "outputId": "6ea5f064-9a4a-4933-aac3-b88c22acb274"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fbNhxI1aE7Ru"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}